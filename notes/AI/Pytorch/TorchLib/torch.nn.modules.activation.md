
- 激活函数层


# 概述
---
- 激活函数层对特征进行非线性变换，赋予多层神经网络具有深度的意义



# 实现
---
## nn.Sigmoid
- 计算公式
$$
y=\frac{1}{1+e^{-x}}
$$
- 梯度公式
$$
y'=y*(1-y)
$$
- 特性
	- 输出在(0,1)，符合概率
	- 导数在$[0,0.25]$，易导致梯度消失
	- 输出为非0均值，破坏数据分布


## nn.tanh
- 计算公式
$$
y=\frac{\sinh {x}}{\cosh x}=\frac{{e^x-e^{-x}}}{e^x+e^{-x}}=\frac{2}{1+e^{-2x}}+1
$$
- 梯度公式
$$
y'=1-y^2
$$
- 特性
	- 输出在(-1,1)，符合0均值
	- 导数范围在(0,1)，易导致梯度消失

## nn.ReLU
- 修正线性单元
- 计算公式：
$$
y=max\{0,x\}
$$
- 梯度公式
- 特性：
	- 输出均为非负数，负半轴导致死神经元
	- 导数是1，缓解梯度消失，但易引发梯度爆炸

### 改进
#### nn.LeakyReLU
- 增加负半轴斜率
- 参数
	- slope
#### nn.PReLU
- 负半轴斜率可学习
- 参数
	- 初始化init

#### nn.RReLU
- 负半轴斜率随机
- 参数
	- lower
	- upper